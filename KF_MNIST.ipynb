{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "azKzys_66G42"
   },
   "source": [
    "# Scaling Jupyter Notebook with Kubeflow Pipelines\n",
    "\n",
    "In this notebook, you will write a Machine Learning (ML) model using a Jupyter notebook and scale it with a [Kubeflow pipeline](https://www.kubeflow.org/docs/pipelines/overview/pipelines-overview/).\n",
    "\n",
    "**The focus of the notebook is not on how to write an ML classifier. This notebook explains how notebooks can be integrated with Kubeflow pipelines.**\n",
    "\n",
    "You will use the [basic classification with fashion MNIST and Tensorflow](https://www.tensorflow.org/tutorials/keras/classification) to test it.\n",
    "\n",
    "_Please note this notebook should be run on a [notebook server](https://www.kubeflow.org/docs/notebooks/) within Kubeflow._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kN5XA9ybEtwU"
   },
   "source": [
    "## Classify Clothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KJ4EgzcMEUko"
   },
   "source": [
    "The [Fashion MNIST](https://github.com/zalandoresearch/fashion-mnist) dataset contains 70,000 grayscale images in 10 clothing categories. Each image is 28x28 pixels in size.\n",
    "\n",
    "_Can you write an model that when given an image it can tell what kind of clothing it is?_\n",
    "\n",
    "That's exactly what you will build when you complete this notebook.\n",
    "\n",
    "You might have [seen this example before](https://www.tensorflow.org/tutorials/keras/classification) as it's a popular classifier, well suited to learning how to use Tensorflow and machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xMIxLNEiGR3x"
   },
   "source": [
    "<table>\n",
    "  <tr><td>\n",
    "    <img src=\"https://tensorflow.org/images/fashion-mnist-sprite.png\"\n",
    "         alt=\"Fashion MNIST sprite\"  width=\"600\">\n",
    "  </td></tr>\n",
    "  <tr><td align=\"center\">\n",
    "    <b>Figure 1.</b> <a href=\"https://github.com/zalandoresearch/fashion-mnist\">Fashion-MNIST samples</a> (by Zalando, MIT License).<br/>&nbsp;\n",
    "  </td></tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before you start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should install a few packages to explore the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!python -m pip install --user --upgrade pip\n",
    "!pip install --user --upgrade pandas matplotlib numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should restart kernel for the changes to take effect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's time to write the model. You should import the following libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "y4DB_1u5H1fT",
    "outputId": "58a8d65c-e978-4af2-aa85-114d4a7f3f29",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TensorFlow and tf.keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Data exploration libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OW5-_PC3H3YB"
   },
   "source": [
    "You should import the fashion MNIST dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HrDlsMh4LoXz"
   },
   "outputs": [],
   "source": [
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t9FDsUlxCaWW"
   },
   "source": [
    "In the dataset, each image is mapped to a label.\n",
    "\n",
    "You can imagine having a picture of a shirt and the string \"shirt\" next to it.\n",
    "\n",
    "In other words, someone else did the hard work of mapping images and labels (at least part of it).\n",
    "\n",
    "_But there's a gotcha._\n",
    "\n",
    "Instead of expressing the labels as words, the dataset has a mapping of image-integer.\n",
    "\n",
    "Like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "pd.DataFrame(class_names, columns=[\"Cloth type\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1Y_88GQbMX1G"
   },
   "source": [
    "\n",
    "_How many images are available as training data?_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of training images: {train_images.shape[0]}\\n')\n",
    "print(f'Number of labels: {len(train_labels)}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_How many images are available as testing data?_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of test images: {test_images.shape[0]}\\n')\n",
    "print(f'Number of test labels: {len(test_labels)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's the size of the images?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "id": "e2-Zbu0BMpt1",
    "outputId": "51310a2d-ad13-4127-f1eb-db3237524044",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(f'Image size: {train_images.shape[1:]}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The images are in grayscale:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "for i in range(25):\n",
    "    plt.subplot(5,5,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(train_images[i], cmap=plt.cm.binary)\n",
    "    plt.xlabel(class_names[train_labels[i]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DVOPKTgdP-nS"
   },
   "source": [
    "The grayscale goes from 0 (black) to (255) white.\n",
    "\n",
    "> Please notice that, while working with the model, you will have to normalise the values and make sure they are between 0 and 1.\n",
    "\n",
    "Now that you have familiriased with the data, let's have a look at how to train the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "igI1dvp7SWWV"
   },
   "source": [
    "## Building a Kubeflow pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_ctN1VrL_qnA"
   },
   "source": [
    "The training model should have two functions:\n",
    "\n",
    "- a function that ingests (picture, label) pairs and trains a model\n",
    "- a function that, given an image, predicts the label\n",
    "\n",
    "[You can learn how the model works in this detailed tutorial](https://www.tensorflow.org/tutorials/keras/classification). For now knowing that there are two functions is enough to move to the next step.\n",
    "\n",
    "Since you will run the model on Kubeflow, you should install the [Kubeflow Pipelines SDK](https://www.kubeflow.org/docs/pipelines/sdk/sdk-overview/) package.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "kWutHs306En6",
    "outputId": "f87b05b5-0555-43ba-8dae-04056821fe1b",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!pip install --user --upgrade kfp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eBwaov51AFex"
   },
   "source": [
    "You should restart kernel for changes to take effect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "colab_type": "code",
    "id": "iUJZqAuN6EoK",
    "outputId": "8d520295-afdd-456f-a6f4-d9a17fda8ce2"
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RMWQNog7AZFP"
   },
   "source": [
    "If the installation was successful you should be able to see the following file:\n",
    "\n",
    "**/usr/local/bin/dsl-compile**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LmHw4UGN6EoX"
   },
   "outputs": [],
   "source": [
    "!which dsl-compile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "abg3ycFVFhBC"
   },
   "source": [
    "You should import the Kubeflow Pipelines SDK in your project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "import kfp.dsl as dsl\n",
    "import kfp.components as comp\n",
    "from typing import NamedTuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first function to be run in the pipeline is the `train` function.\n",
    "\n",
    "As you can imagine, here the model is trained using the test data.\n",
    "\n",
    "Let's focus on the Kubeflow specific features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VpeW1k1o6Eo5"
   },
   "outputs": [],
   "source": [
    "def train(data_path, model_file)-> NamedTuple('output', [('mlpipeline_ui_metadata', 'UI_metadata'), ('mlpipeline_metrics', 'Metrics')]):\n",
    "    \n",
    "    import pickle\n",
    "    import json\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.python import keras\n",
    "    \n",
    "    # Download the dataset and split into training and test data. \n",
    "    fashion_mnist = keras.datasets.fashion_mnist\n",
    "\n",
    "    (train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
    "\n",
    "    # Normalize the data so that the values all fall between 0 and 1.\n",
    "    train_images = train_images / 255.0\n",
    "    test_images = test_images / 255.0\n",
    "\n",
    "    # Define the model using Keras.\n",
    "    model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    keras.layers.Dense(10)\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    # Run a training job with specified number of epochs\n",
    "    model.fit(train_images, train_labels, epochs=10)\n",
    "\n",
    "    # Evaluate the model and print the results\n",
    "    test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)\n",
    "    print('Test accuracy:', test_acc)\n",
    "\n",
    "    # Save the model to the designated \n",
    "    model.save(f'{data_path}/{model_file}')\n",
    "\n",
    "    # Save the test_data as a pickle file to be used by the predict component.\n",
    "    with open(f'{data_path}/test_data', 'wb') as f:\n",
    "        pickle.dump((test_images,test_labels), f)\n",
    "    \n",
    "    metadata = {\n",
    "        'outputs' : [{\n",
    "          'type': 'web-app',\n",
    "          'storage': 'inline',\n",
    "          'source': \"<div>Done</div>\",\n",
    "        }]\n",
    "      }\n",
    "\n",
    "    metrics = {\n",
    "      'metrics': [{\n",
    "          'name': 'Accuracy',\n",
    "          'numberValue':  float(test_acc),\n",
    "        }, {\n",
    "          'name': 'Loss',\n",
    "          'numberValue':  float(test_loss),\n",
    "        }]}\n",
    "    \n",
    "    from collections import namedtuple\n",
    "    print_output = namedtuple('output', ['mlpipeline_ui_metadata', 'mlpipeline_metrics'])\n",
    "    return print_output(json.dumps(metadata), json.dumps(metrics))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should notice that the return value of the function is annotated with a NamedTuple.\n",
    "\n",
    "The NamedTuple is used to return multiple outputs.\n",
    "\n",
    "In this case, the two output are:\n",
    "\n",
    "1. [Metadata](https://www.kubeflow.org/docs/components/metadata/) which is used to [visualise the output](https://www.kubeflow.org/docs/pipelines/sdk/output-viewer/) of the current step.\n",
    "1. [Metrics](https://www.kubeflow.org/docs/pipelines/sdk/pipelines-metrics/) which collects the metrics for the current step.\n",
    "\n",
    "You can have more outputs, these are convenient when you wish to have a quick visual clue of what the steps were about. An example of one is shown below:\n",
    "\n",
    "![Output view in Kubeflow pipelines](https://www.kubeflow.org/docs/images/taxi-tip-run-output.png)\n",
    "\n",
    "The `predict` function is similar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_pxGKEae6Eo_"
   },
   "outputs": [],
   "source": [
    "def predict(data_path, model_file, image_number)-> NamedTuple('output', [('mlpipeline_ui_metadata', 'UI_metadata'), ('mlpipeline_metrics', 'Metrics')]):\n",
    "    \n",
    "    import pickle\n",
    "    import json\n",
    "    \n",
    "    import sys, subprocess;\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'Pillow'])\n",
    "    import base64\n",
    "    from PIL import Image\n",
    "    from io import BytesIO\n",
    "\n",
    "    import tensorflow as tf\n",
    "    from tensorflow import keras\n",
    "\n",
    "    import numpy as np\n",
    "    \n",
    "    # Load the saved Keras model\n",
    "    model = keras.models.load_model(f'{data_path}/{model_file}')\n",
    "\n",
    "    # Load and unpack the test_data\n",
    "    with open(f'{data_path}/test_data','rb') as f:\n",
    "        test_data = pickle.load(f)\n",
    "    # Separate the test_images from the test_labels.\n",
    "    test_images, test_labels = test_data\n",
    "    # Define the class names.\n",
    "    class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "                   'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "    # Define a Softmax layer to define outputs as probabilities\n",
    "    probability_model = tf.keras.Sequential([model, \n",
    "                                            tf.keras.layers.Softmax()])\n",
    "\n",
    "    # See https://github.com/kubeflow/pipelines/issues/2320 for explanation on this line.\n",
    "    image_number = int(image_number)\n",
    "\n",
    "    # Grab an image from the test dataset.\n",
    "    selected_image = test_images[image_number]\n",
    "\n",
    "    # Add the image to a batch where it is the only member.\n",
    "    img = (np.expand_dims(selected_image,0))\n",
    "\n",
    "    # Predict the label of the image.\n",
    "    predictions = probability_model.predict(img)\n",
    "\n",
    "    # Take the prediction with the highest probability\n",
    "    prediction = np.argmax(predictions[0])\n",
    "\n",
    "    # Retrieve the true label of the image from the test labels.\n",
    "    true_label = test_labels[image_number]\n",
    "    \n",
    "    class_prediction = class_names[prediction]\n",
    "    confidence = 100*np.max(predictions)\n",
    "    actual = class_names[true_label]\n",
    "    \n",
    "    \n",
    "    with open(f'{data_path}/result.txt', 'w') as result:\n",
    "      result.write(\" Prediction: {} | Confidence: {:2.0f}% | Actual: {}\".format(class_prediction,\n",
    "                                                                                confidence,\n",
    "                                                                                actual))\n",
    "\n",
    "    PIL_image = Image.fromarray(np.uint8(selected_image * 255)).convert('RGB')                                                                            \n",
    "    buffered = BytesIO()\n",
    "    PIL_image.save(buffered, format=\"JPEG\")\n",
    "    img_str = base64.b64encode(buffered.getvalue()).decode()\n",
    "    metadata = {\n",
    "        'outputs' : [{\n",
    "          'type': 'web-app',\n",
    "          'storage': 'inline',\n",
    "          'source': \"<div>Input: <img width=\\\"200\\\" src=\\\"data:image/jpeg;base64,{}\\\"/></div><p>Prediction: {}</p>\".format(img_str, actual),\n",
    "        }]\n",
    "      }\n",
    "\n",
    "    metrics = {\n",
    "      'metrics': [{\n",
    "          'name': 'Confidence',\n",
    "          'numberValue': confidence,\n",
    "        }]}\n",
    "\n",
    "    from collections import namedtuple\n",
    "    print_output = namedtuple('output', ['mlpipeline_ui_metadata', 'mlpipeline_metrics'])\n",
    "    return print_output(json.dumps(metadata), json.dumps(metrics))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It uses the same NamedTuple to export multiple output: Metadata and Metrics.\n",
    "\n",
    "Notice how the output includes the image used for prediction and the actual prediction.\n",
    "\n",
    "A part from the NamedTuple, the two functions are just two regular Python functions.\n",
    "\n",
    "However, in a Kubeflow Pipeline, each function runs in a separate container.\n",
    "\n",
    "_How can you pass arguments to a function that lives in another container?_\n",
    "\n",
    "_Also, how do you specify which container image it should use?_\n",
    "\n",
    "You can decorate your functions with `func_to_container_op` to:\n",
    "\n",
    "1. Tell Kubeflow that this is a function that can be run as part of Kubeflow Pipelines.\n",
    "1. Specify in which container the function should run.\n",
    "\n",
    "`func_to_container_op` is also taking care of function arguments and return values so that when the [Pod](https://kubernetes.io/docs/concepts/workloads/pods/pod/) starts it receives the incoming data from standard input and it writes to standard output.\n",
    "\n",
    "This step is crucial since the two functions run in different Python runtime environments.\n",
    "\n",
    "> If you're interested in exploring how this works, you should know that the arguments and output are serialised/deserialised for every invocation. You can [learn more about the process here.](https://towardsdatascience.com/a-dummies-guide-to-build-a-kubeflow-pipeline-c1f61160cba6)\n",
    "\n",
    "And here's `func_to_container_op` in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_op = comp.func_to_container_op(train, base_image='tensorflow/tensorflow:2.2.0rc2-py3')\n",
    "predict_op = comp.func_to_container_op(predict, base_image='tensorflow/tensorflow:2.2.0rc2-py3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have now wrapped the two functions to run in thier respective containers. However, there is still no information on which order these those two steps should be executed.\n",
    "\n",
    "_Sounds like a job for pipeline_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '/mnt'\n",
    "MODEL_PATH='mnist_model.h5'\n",
    "# An integer representing an image from the test set that the model will attempt to predict the label for.\n",
    "IMAGE_NUMBER = 0\n",
    "\n",
    "# Define the pipeline\n",
    "@dsl.pipeline(\n",
    "   name='Fashion MNIST Pipeline',\n",
    "   description='A pipeline that performs fashion MNIST model training and prediction.'\n",
    ")\n",
    "\n",
    "# Define parameters to be fed into pipeline\n",
    "def mnist_container_pipeline(\n",
    "    data_path: str = DATA_PATH,\n",
    "    model_file: str = MODEL_PATH, \n",
    "    image_number: int = IMAGE_NUMBER\n",
    "):\n",
    "    \n",
    "    # 1. Define volume to share data between components.\n",
    "    vop = dsl.VolumeOp(\n",
    "    name=\"create_volume\",\n",
    "    resource_name=\"data-volume\", \n",
    "    size=\"1Gi\", \n",
    "    modes=dsl.VOLUME_MODE_RWO)\n",
    "    \n",
    "    # 2. Create MNIST training component.\n",
    "    mnist_training_container = train_op(data_path, model_file) \\\n",
    "                                    .add_pvolumes({data_path: vop.volume})\n",
    "\n",
    "    # 3. Create MNIST prediction component.\n",
    "    mnist_predict_container = predict_op(data_path,\n",
    "                                         model_file,\n",
    "                                         image_number\n",
    "                                         ) \\\n",
    "                                    .add_pvolumes({data_path: mnist_training_container.pvolume})\n",
    "    \n",
    "    # 4. Print the result of the prediction\n",
    "    mnist_result_container = dsl.ContainerOp(\n",
    "        name=\"print_prediction\",\n",
    "        image='library/bash:4.4.23',\n",
    "        pvolumes={data_path: mnist_predict_container.pvolume},\n",
    "        arguments=['cat', f'{data_path}/result.txt']\n",
    "    )\n",
    "\n",
    "    vop.delete().after(mnist_result_container)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pipeline is just another function which is annotated with [`@dsl.pipeline`](https://www.kubeflow.org/docs/pipelines/sdk/sdk-overview/#sdk-packages).\n",
    "\n",
    "Inside the function there are 4 main blocks:\n",
    "\n",
    "1. In the first block, a [VolumeOp](https://www.kubeflow.org/docs/pipelines/sdk/manipulate-resources/#persistent-volume-claims-pvcs) is created to persist the data. In the background, Kubeflow creates a Persistent Volume Claim (PVC) and Persistent Volume (PV).\n",
    "1. The second block is the training function. Notice how it uses the wrapped function. Also, the volume is attached to it.\n",
    "1. The predict block comes third and it is similar to the previous one. You still have to attach the volume to pass the data to it.\n",
    "1. The last block is a container that display the result. Since the result is stored in the volume, the last block has to mount the volume.\n",
    "\n",
    "_How do you submit the pipeline to the cluster?_\n",
    "\n",
    "You could put all of this pipline code in `pipeline.py`, compile it using [`dsl-compile`](https://www.kubeflow.org/docs/pipelines/sdk/build-component/#compile-the-pipeline) and upload the compiled artefact to Kubeflow Pipelines via dashboard.\n",
    "\n",
    "But there's a better way.\n",
    "\n",
    "You can connect to the Kubeflow Pipelines API server with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = kfp.Client(host='ml-pipeline.kubeflow.svc.cluster.local:8888')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b1pqw3d0tnth"
   },
   "source": [
    "The host is the name of the Kubernetes Service that is exposed by the API and it is available only iniside the cluster.\n",
    "\n",
    "The last step is to use the client to submit the pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8cFLEOyq6Ep5",
    "outputId": "ac3009e4-4beb-4486-bc0a-ad30dfd2780f"
   },
   "outputs": [],
   "source": [
    "pipeline_func = mnist_container_pipeline\n",
    "experiment_name = 'fashion_minist_kubeflow'\n",
    "run_name = pipeline_func.__name__ + ' run'\n",
    "\n",
    "arguments = {\"data_path\":DATA_PATH,\n",
    "             \"model_file\":MODEL_PATH,\n",
    "             \"image_number\": IMAGE_NUMBER}\n",
    "\n",
    "# Submit pipeline directly from pipeline function\n",
    "run_result = client.create_run_from_pipeline_func(pipeline_func, \n",
    "                                                  experiment_name=experiment_name, \n",
    "                                                  run_name=run_name, \n",
    "                                                  arguments=arguments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see two links to inspect the pipeline.\n",
    "\n",
    "Congratulations, you just built a Kubeflow Pipeline!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "KF Fashion MNIST",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}